from bs4 import BeautifulSoup
import requests
import json
import numpy
import pandas as pd
import tabula as tb

headers = {
    'Accept' : '*/*',
    'Accept-Encoding' : 'gzip, deflate, br',
    'Accept-Language' : 'en-US,en;q=0.9',
    'Connection' : 'keep-alive',
    'Content-Length' : '44',
    'Content-Type' : 'application/x-www-form-urlencoded; charset=UTF-8',
    #'Cookie' : '_ga=GA1.2.1911645244.1611602763; _gid=GA1.2.1141854335.1611700927',
    'Host' : 'disclosures-clerk.house.gov',
    'Origin' : 'https://disclosures-clerk.house.gov',
    'Referer' : 'https://disclosures-clerk.house.gov/PublicDisclosure/FinancialDisclosure',
    'sec-ch-ua' : '"Google Chrome";v="87", " Not;A Brand";v="99", "Chromium";v="87"',
    'sec-ch-ua-mobile' : '?0',
    'Sec-Fetch-Dest' : 'empty',
    'Sec-Fetch-Mode' : 'cors',
    'Sec-Fetch-Site' : 'same-origin',
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',
    'X-Requested-With' : 'XMLHttpRequest'
}

data = {
    'LastName' : 'pelosi',
    'FilingYear' : '',
    'State' : '',
    'District' : ''
}

r = requests.post("https://disclosures-clerk.house.gov/PublicDisclosure/FinancialDisclosure/ViewMemberSearchResult", headers = headers, data = data)

linksoup = BeautifulSoup(r.content, "html.parser")

list = []
dflist = []

for a in linksoup.find_all('a', href=True):

    
    list.append(a['href'])
    r2 = requests.get("https://disclosures-clerk.house.gov/" + a['href'] )
    dflist.append(tb.read_pdf(r2.content))
    
print(list)
